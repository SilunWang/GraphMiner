
\subsection{Papers read by Yuwei Zhang }
\textbf{Evaluating cooperation in communities with k-core structure}
\cite{Evaluating}
\begin{itemize*}
\item {\em Problem Definition}: The paper focuses on community detection and evalutation, which means dense connections among some of the nodes. The author make some novel changes to the k-core concepts including updating mtric for evaluating cohesiveness, assigning weights on the edges and other extended experimental evaluation.
\item {\em Summary}:The original k-cores algorithm keeps deleting nodes whose degree are less than k and thus take the number of each set of vertices in the subgraph to do the evalution, which fails in the case where many co-authors have equal weight. This paper improves the method to define a co-authorship edge weight instead and recompute the evalution metrics with restrictions considered. In the expriment stage, testing on an unfiltered graph turns out to be extremely biased while on a filtered one(those co-author a lot) the results seem to be resonable. When weights graph method is applied, the extrem cases where k is too big are ignored and the algorithm gives better results. 
\item {\em Shortcomings}:There is no standard metrics to evaluate these algorithms/methods propsed in the paper. Also when we consider the graphs as social networks, where the relationship between two nodes are more than just co-author, for instance we have flollow, like, dislike, the weighted method should be furthur adjusted and it might be hard to derive the best weighting formula.
\end{itemize*}

\textbf{Vertexica: your relational friends for graph analytics}
\cite{vertexica}
\begin{itemize*}
\item {\em Problem Definition}: To build a graph analysis tool, Vertexica, on top of a rdb that supports vertex-centric query interface. The system leverage the realational features and enable better graph analysis.
\item {\em Summary}:Vertexica supports user-friendly and high-performance graph analysis by injecting data storage, query processing and query interfaces and supports various kinds of relational database. The system consists of four main components: physical storage to store data, coordinator as the center management driver, worker as the conntainer for the computation programs and vertex computation to process user queries. Vertexica also take several optimization techniques including: table union instead of table join, paralleling workers to work on multicores or multi machines, vertex batching to partition the table and create new tables other than update the orgin information to boost the performance. The paper also includes some use case demonstrations. 
\item {\em Shortcomings}:Hand-coded sql implementations give even better performance in the experiments of the paper. Is it possibel to furthur optimize the performance when using the user-friendly vertex-centric query interface?
\end{itemize*}

\textbf{Visual Exploaration of Collaboration Networks based on Graph Degeneracy}
\cite{visual}
\begin{itemize*}
\item {\em Problem Definition}:To build a system that supports visual exploration of collaboration networks based on ranking of the nodes and filtering methods on the edges. It works on DBLP and is suitable for the large-scale networks.
\item {\em Summary}:The idea of graph degeneracy is derived from the conceptsof k-cores, which is introduced in previous paper(the one just summarized). Basically in this system, it extracts the co-authorship graph using the algorithm described in the other paper using filtered weighted egde algorithm, and then partition the graph to f-cores based on the Trim process. Then comes the ranking, by repeatedly performing the Trim procedure to remove more vertexes and in the end stores in the relation databse for furthur query. The system can be useful to demonstrate bibliographic data.
\item {\em Shortcomings}:For huge graphs, the k-core process may be extremly time-consuming. And it will take a long time for the system to reflect the updates in the graph.
\end{itemize*}


\subsection{Papers read by Silun Wang}
The first paper was the Belief Propogation paper by Wolfgang Gatterbauer, Stephan Gunnemann, Danai Koutra, and Christos Faloutsos
\cite{BeliefPro}
\begin{itemize*}
\item {\em Problem Definition}: In big social networks, sometimes we need to infer the labels for particular nodes via transductive inference or semi-supervised learning. The classical belief propagation algorithm is widely used in such scenario, but it does not guarantee convergence in loopy graphs. In this paper, the authors propose Linearized Belief propagation and Single-pass belief propagation which are based on different restrictions and assumptions and much faster than BP.
\item {\em Summary}:
    In a nutshell, LBP and SBP have the following advantages over BS:
    \begin{enumerate}
	\item Have convergence guarantees
	\item Have closed-form solutions, thus reducing computational cost
	\item Can be implemented on standardized SQL
	\item SBP can be updated incrementally
	\end{enumerate}
	LBP requires messages are normalized, thus the final belief matrix can be calculated via elegant matrix operations. SBP is based on the assumption that the impact of inference damps with length of paths. To obtain the final belief matrix, each node and each edge only need to be visited once.
\item {\em Shortcomings}:
      The Daubechies wavelets require a wrap-around setting,
      which may lead to non-intuitive results.
\end{itemize*}

The second paper was the k-core decomposition paper by Ignacio Alvarez-Hamelin, Luca Dall’Asta, Alain Barrat, and Alessandro
\cite{kcore}
\begin{itemize*}
\item {\em Problem Definition}: To visualize large complex networks is a big challenge, especially when you want clarity of graph and maintaining as much information as possible in the meantime. In this paper, the authors present an effective algorithm — k-core decomposition to visualize large complex networks in 2D dimension.
\item {\em Summary}:
K-core decomposition introduces several terms: coreness, shell, cluster. It assigns each vertex a polar coordinate, thus visualizing a large complex network in 2D dimension while preserving relative hierarchical structures, connectivity and clustering properties, as well as interrelationship between hierarchies. What’s more, the overall time complexity is only linear as $O(n + e)$.
\item{\em Shortcomings}:
In order to obtain a readable layout, we need to tune several parameters. Can we learn these parameters automatically? Also, for huge networks, even a k-core decomposed graph seems to be nasty. Future work might need to combine nodes into a cluster and visualize a cluster via a simplified motif representation.
\end{itemize*}

The third paper was the visualization paper by Cody Dunne and Ben Shneiderman
\cite{motif}
\begin{itemize*}
\item{\em Problem Definition}:
Big data explosion results in huge and complex networks. In order to understand the relationship between entities and also individual attributes, traditional statistical charts are not applicable. Node-link diagrams are introduced and quickly excels among others. However, some node-link diagrams require relatively large screen space while containing little or repeated information, and optimization for the layout is NP hard. We need a more simplified visualization method which preserves important information.
\item{\em Summary}:
Authors of this paper on one hand defines three kinds of motifs: fan, connector and clique. A fan consists of a head node connected to leaf nodes with no other neighbors. A connector motif consists of functionally equivalent span nodes that solely link a set of anchor nodes. A clique motif consists of a set of member nodes in which each pair is connected by at least one link. On the other hand, the author presents an effective algorithm for motif detection with polynomial time complexity. For example, they use the obvious algorithm for detecting fan motifs which has a run time complexity of O($|G.nodes|$*average neighbor count) and in order to find all cliques they use the Tomita algorithm. After replacing the motifs with more representative glyphs, the graph requires much less screen space and layout effort. It helps us more easily understand the network and even discover some hidden relationships. 
\item{\em Shortcomings}:
Users need to be trained for a short time to fully understand this new representation. It is ambiguous in choosing clique motifs because they often overlap with each other. Future work could present users with these overlaps and relative confidence on different partitions.
\end{itemize*}
